{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries and functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(r\".\\Dataset\\yelpReviewsDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_int(sentiment):\n",
    "    sentiment_map = {\n",
    "        1: 0,\n",
    "        2: 1,\n",
    "        3: 2,\n",
    "        4: 3,\n",
    "        5: 4\n",
    "    }\n",
    "    return sentiment_map.get(sentiment, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = reviews['Review'].values\n",
    "labels = reviews['Rating'].apply(sentiment_to_int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r\".\\TokenizedDataset\\yelpReviewsDatasetTokens.pkl\", \"rb\") as file:\n",
    "    tokenized_Reviews = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_Reviews, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes=5)\n",
    "y_test = to_categorical(y_test, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "tokenizer_json = word_tokenizer.to_json()\n",
    "with io.open('snn_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75957"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding all reviews to fixed length\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Matrix having 100 columns \n",
    "# Containing 100-dimensional GloVe word embeddings for all words in our corpus.\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75957, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network architecture\n",
    "snn_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=max_length , trainable=False)\n",
    "snn_model.add(embedding_layer)\n",
    "snn_model.add(Flatten())\n",
    "snn_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 100)         7595700   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100000)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 500005    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,095,705\n",
      "Trainable params: 500,005\n",
      "Non-trainable params: 7,595,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model compiling\n",
    "snn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(snn_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.3852 - acc: 0.4259 - val_loss: 1.3639 - val_acc: 0.4481\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 1.2113 - acc: 0.4954 - val_loss: 1.4164 - val_acc: 0.4485\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 1.1656 - acc: 0.5149 - val_loss: 1.4376 - val_acc: 0.4491\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 1.1465 - acc: 0.5242 - val_loss: 1.5087 - val_acc: 0.4448\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 1.1318 - acc: 0.5299 - val_loss: 1.5153 - val_acc: 0.4462\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 1.1167 - acc: 0.5371 - val_loss: 1.5597 - val_acc: 0.4472\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.1079 - acc: 0.5396 - val_loss: 1.5990 - val_acc: 0.4486\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.1027 - acc: 0.5425 - val_loss: 1.6154 - val_acc: 0.4478\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.0912 - acc: 0.5483 - val_loss: 1.6655 - val_acc: 0.4459\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 1.0896 - acc: 0.5502 - val_loss: 1.7258 - val_acc: 0.4404\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "snn_model_history = snn_model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 7s 5ms/step - loss: 1.7190 - acc: 0.4460\n",
      "Test Score: 1.7189948558807373\n",
      "Test Accuracy: 0.4459800124168396\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the Test Set\n",
    "score = snn_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 6s 4ms/step\n",
      "[[0.58652793 0.27525387 0.11007752 0.03352404 0.03295374]\n",
      " [0.25520263 0.38464279 0.2514581  0.10739737 0.0566361 ]\n",
      " [0.09611172 0.218956   0.32942045 0.22954933 0.09160536]\n",
      " [0.03030303 0.07999287 0.2118863  0.37488708 0.26282374]\n",
      " [0.03185469 0.04115446 0.09715762 0.25464218 0.55598105]]\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "y_prediction = snn_model.predict(X_test)\n",
    "y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN accuracy is 44.60%\n",
      "------------------------------------------------\n",
      "Confusion Matrix:\n",
      "      0     1     2     3     4\n",
      "0  6426  1545  1491   334   327\n",
      "1  2796  2159  3406  1070   562\n",
      "2  1053  1229  4462  2287   909\n",
      "3   332   449  2870  3735  2608\n",
      "4   349   231  1316  2537  5517\n",
      "------------------------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61     10123\n",
      "           1       0.38      0.22      0.28      9993\n",
      "           2       0.33      0.45      0.38      9940\n",
      "           3       0.37      0.37      0.37      9994\n",
      "           4       0.56      0.55      0.56      9950\n",
      "\n",
      "    accuracy                           0.45     50000\n",
      "   macro avg       0.45      0.45      0.44     50000\n",
      "weighted avg       0.45      0.45      0.44     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = metrics.accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print('SNN accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')\n",
    "print('------------------------------------------------')\n",
    "print('Confusion Matrix:')\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_prediction)))\n",
    "print('------------------------------------------------')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
