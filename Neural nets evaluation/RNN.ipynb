{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries and functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import array\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(r\".\\Dataset\\yelpReviewsDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_int(sentiment):\n",
    "    sentiment_map = {\n",
    "        1: 0,\n",
    "        2: 1,\n",
    "        3: 2,\n",
    "        4: 3,\n",
    "        5: 4\n",
    "    }\n",
    "    return sentiment_map.get(sentiment, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = reviews['Review'].values\n",
    "labels = reviews['Rating'].apply(sentiment_to_int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r\".\\TokenizedDataset\\yelpReviewsDatasetTokens.pkl\", \"rb\") as file:\n",
    "    tokenized_Reviews = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_Reviews, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes=5)\n",
    "y_test = to_categorical(y_test, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "tokenizer_json = word_tokenizer.to_json()\n",
    "with io.open('rnn_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75957"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding all reviews to fixed length\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Matrix having 100 columns \n",
    "# Containing 100-dimensional GloVe word embeddings for all words in our corpus.\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75957, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Network (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "# Neural Network architecture\n",
    "lstm_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(LSTM(128))\n",
    "lstm_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 100)         7595700   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,713,593\n",
      "Trainable params: 117,893\n",
      "Non-trainable params: 7,595,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model compiling\n",
    "lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 10166s 8s/step - loss: 1.6096 - acc: 0.1992 - val_loss: 1.6095 - val_acc: 0.2008\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 6986s 6s/step - loss: 1.6096 - acc: 0.1990 - val_loss: 1.6095 - val_acc: 0.2009\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 6238s 5s/step - loss: 1.6095 - acc: 0.1991 - val_loss: 1.6096 - val_acc: 0.1982\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 6140s 5s/step - loss: 1.6095 - acc: 0.2010 - val_loss: 1.6097 - val_acc: 0.1989\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 5994s 5s/step - loss: 1.6095 - acc: 0.1998 - val_loss: 1.6094 - val_acc: 0.2009\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 5977s 5s/step - loss: 1.6095 - acc: 0.2009 - val_loss: 1.6095 - val_acc: 0.2009\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 8813s 7s/step - loss: 1.6095 - acc: 0.1992 - val_loss: 1.6095 - val_acc: 0.2011\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 10588s 8s/step - loss: 1.6095 - acc: 0.1999 - val_loss: 1.6095 - val_acc: 0.1989\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 10741s 9s/step - loss: 1.6095 - acc: 0.1994 - val_loss: 1.6094 - val_acc: 0.2011\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 9499s 8s/step - loss: 1.6095 - acc: 0.1990 - val_loss: 1.6095 - val_acc: 0.1982\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 442s 283ms/step - loss: 1.6095 - acc: 0.1990\n",
      "Test Score: 1.6094543933868408\n",
      "Test Accuracy: 0.19900000095367432\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the Test Set\n",
    "score_lstm = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test Score:\", score_lstm[0])\n",
    "print(\"Test Accuracy:\", score_lstm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 402s 257ms/step\n",
      "[[0.      0.      0.      0.      0.20246]\n",
      " [0.      0.      0.      0.      0.19986]\n",
      " [0.      0.      0.      0.      0.1988 ]\n",
      " [0.      0.      0.      0.      0.19988]\n",
      " [0.      0.      0.      0.      0.199  ]]\n"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "y_prediction = lstm_model.predict(X_test)\n",
    "y_prediction = np.argmax(y_prediction, axis = 1)\n",
    "y_test=np.argmax(y_test, axis=1)\n",
    "#Create confusion matrix and normalizes it over predicted (columns)\n",
    "result = confusion_matrix(y_test, y_prediction , normalize='pred')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN accuracy is 19.90%\n",
      "------------------------------------------------\n",
      "Confusion Matrix:\n",
      "   0  1  2  3      4\n",
      "0  0  0  0  0  10123\n",
      "1  0  0  0  0   9993\n",
      "2  0  0  0  0   9940\n",
      "3  0  0  0  0   9994\n",
      "4  0  0  0  0   9950\n",
      "------------------------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     10123\n",
      "           1       0.00      0.00      0.00      9993\n",
      "           2       0.00      0.00      0.00      9940\n",
      "           3       0.00      0.00      0.00      9994\n",
      "           4       0.20      1.00      0.33      9950\n",
      "\n",
      "    accuracy                           0.20     50000\n",
      "   macro avg       0.04      0.20      0.07     50000\n",
      "weighted avg       0.04      0.20      0.07     50000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\PythonProjects\\NeuralNetsEvaluation\\splenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Projects\\PythonProjects\\NeuralNetsEvaluation\\splenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Projects\\PythonProjects\\NeuralNetsEvaluation\\splenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = metrics.accuracy_score(y_prediction, y_test)\n",
    "\n",
    "print('RNN accuracy is',str('{:04.2f}'.format(accuracy_score*100))+'%')\n",
    "print('------------------------------------------------')\n",
    "print('Confusion Matrix:')\n",
    "print(pd.DataFrame(confusion_matrix(y_test, y_prediction)))\n",
    "print('------------------------------------------------')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
